# -*- coding: utf-8 -*-
"""House Pricing - Paulo Implementation (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sW1hc2RDPEnT6q6hmuBT19fv3FR2OjtP
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
# Any results you write to the current directory are saved as output.

# Commented out IPython magic to ensure Python compatibility.
# invite people for the Kaggle party
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import norm, skew
from sklearn.preprocessing import StandardScaler
from scipy import stats
import warnings

warnings.filterwarnings('ignore')
import statsmodels.api as sm
from sklearn.preprocessing import LabelEncoder

# %matplotlib inline

color = sns.color_palette()
sns.set_style('darkgrid')
import warnings


def ignore_warn(*args, **kwargs):
    pass


warnings.warn = ignore_warn  # ignore annoying warning (from sklearn and seaborn)
pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))  # Limiting floats output to 3 decimal points
from subprocess import check_output

# bring in the six packs
df_train = pd.read_csv('data/train.csv')
df_test = pd.read_csv('data/test.csv')
df_train['data_origin'] = 'Train'
df_test['data_origin'] = 'Test'
df_all = pd.concat([df_train, df_test], axis=0)
# check the decoration

df_temp = pd.read_csv('data/train.csv')
skew(df_temp['SalePrice'])

df_all['SalePrice'].isnull().sum()

mu, sigma = norm.fit(df_train['SalePrice'])
sns.distplot(df_train['SalePrice'], fit=norm)

# Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

# Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(df_train['SalePrice'], plot=plt)
plt.show()

df_train['SalePrice'] = np.log(df_train['SalePrice'])
sns.distplot(df_train['SalePrice'], fit=norm)

# Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')
plt.ylabel('Frequency')
plt.title('SalePrice distribution')

# Get also the QQ-plot
fig = plt.figure()
res = stats.probplot(df_train['SalePrice'], plot=plt)
plt.show()

df_all.loc[df_all['data_origin'] == 'Train', 'SalePrice'] = df_train['SalePrice']

y = df_train.SalePrice
df_train.drop(['SalePrice'], axis=1, inplace=True)
df_all.drop(['SalePrice'], axis=1, inplace=True)

"""# Missing Data"""

# missing data
plt.figure(figsize=(14, 12))
total = df_all.isnull().sum().sort_values(ascending=False)
percent = (df_all.isnull().sum() / df_all.shape[0]).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data = missing_data[missing_data['Total'] > 0]

sns.barplot(y=missing_data.index, x='Percent', data=missing_data, orient='h')

missing_data

# PoolQC: Pool quality, in the description says that NA means no pool
df_all['PoolQC'] = df_all['PoolQC'].fillna('NA')

# MiscFeature
df_all['MiscFeature'] = df_all['MiscFeature'].fillna('NA')

# Alley
df_all["Alley"] = df_all["Alley"].fillna("NA")

# Fence
df_all["Fence"] = df_all["Fence"].fillna("NA")

# FireplaceQu
df_all["FireplaceQu"] = df_all["FireplaceQu"].fillna("NA")

# 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'.
for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):
    df_all[col] = df_all[col].fillna('NA')

# 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'
for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):
    df_all[col] = df_all[col].fillna('NA')

df_all["MasVnrType"] = df_all["MasVnrType"].fillna("NA")

# 'GarageYrBlt', 'GarageArea', 'GarageCars'. Replacing by zero because its a numeric field
for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):
    df_all[col] = df_all[col].fillna(0)

# 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'. Replacing by zero
for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):
    df_all[col] = df_all[col].fillna(0)

df_all["MasVnrArea"] = df_all["MasVnrArea"].fillna(0)

# LotFrontage: Linear feet of street connected to property. One good idea is to fill with median of neighborhood
df_all['LotFrontage'] = df_all.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))

# Identifies the general zoning classification of the sale. First replace by mode of Neighborhood
df_all['MSZoning'] = df_all.groupby('Neighborhood')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))

# Utilities -  All records are "AllPub", except for one "NoSeWa"  and 2 NA . Since the house with 'NoSewa' is in the training set, **this feature won't help in predictive modelling**. We can then safely  remove it.

df_all = df_all.drop(['Utilities'], axis=1)

# Functional: Home functionality (Assume typical unless deductions are warranted)
df_all["Functional"] = df_all["Functional"].fillna("Typ")

# Electrical: Just one NA value, lets assume the mode.
df_all['Electrical'] = df_all['Electrical'].fillna(df_all['Electrical'].mode()[0])

# KitchenQual: Kitchen quality. Just one missing data
df_all['KitchenQual'] = df_all['KitchenQual'].fillna(df_all['KitchenQual'].mode()[0])

# Exterior1st: Exterior covering on house
df_all['Exterior1st'] = df_all['Exterior1st'].fillna(df_all['Exterior1st'].mode()[0])
df_all['Exterior2nd'] = df_all['Exterior2nd'].fillna(df_all['Exterior2nd'].mode()[0])

# SaleType: Type of sale
df_all['SaleType'] = df_all['SaleType'].fillna(df_all['SaleType'].mode()[0])

# MSSubClass: Identifies the type of dwelling involved in the sale. Probably no bulding class.
df_all['MSSubClass'] = df_all['MSSubClass'].fillna("None")

# Checking if there are null values.
df_all.isnull().sum().sort_values(ascending=False).head(5)

"""#### No missing values!

# Category Transformation
"""

for col in ['MSSubClass', 'OverallCond', 'YrSold', 'MoSold']:
    df_all[col] = df_all[col].astype(str)

columns = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond',
           'ExterQual', 'ExterCond', 'HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1',
           'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',
           'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond',
           'YrSold', 'MoSold')

for col in columns:
    label = LabelEncoder()
    label.fit(df_all[col])
    df_all[col] = label.transform(df_all[col])

# Adding total sqfootage feature
df_all['TotalSF'] = df_all['TotalBsmtSF'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']

df_all.head()

"""## Tratando skewness"""

# Primeiro guardo o nome das colunas numÃ©ricas para testar
all_numeric_cols = df_all.dtypes[df_all.dtypes != 'object'].index

# skewness = 0 : normally distributed.
# skewness > 0 : more weight in the left tail of the distribution.
# skewness < 0 : more weight in the right tail of the distribution.
df_all[all_numeric_cols].apply(lambda x: skew(x)).sort_values(ascending=False)

from scipy.special import boxcox1p

for col in all_numeric_cols:
    df_all[col] = boxcox1p(df_all[col], .15)

df_all = pd.get_dummies(df_all)

train_size = df_train.shape[0]
df_train = df_all[:train_size]
df_test = df_all[train_size:]

"""## Modelling"""

from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
import xgboost as xgb
import lightgbm as lgb

"""#### Lasso Regression"""

df_all.head()

"""-  **LASSO  Regression**  : 

This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's  **Robustscaler()**  method on pipeline
"""

lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))

"""- **Elastic Net Regression** :

again made robust to outliers
"""

ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))

"""- **Kernel Ridge Regression** :"""

KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)

"""- **Gradient Boosting Regression** :

With **huber**  loss that makes it robust to outliers
"""

GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,
                                   max_depth=4, max_features='sqrt',
                                   min_samples_leaf=15, min_samples_split=10,
                                   loss='huber', random_state=5)

"""- **XGBoost** :"""

model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,
                             learning_rate=0.05, max_depth=3,
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state=7, nthread=-1)

"""- **LightGBM** :"""

model_lgb = lgb.LGBMRegressor(objective='regression', num_leaves=5,
                              learning_rate=0.05, n_estimators=720,
                              max_bin=55, bagging_fraction=0.8,
                              bagging_freq=5, feature_fraction=0.2319,
                              feature_fraction_seed=9, bagging_seed=9,
                              min_data_in_leaf=6, min_sum_hessian_in_leaf=11)


def rmse_cv(model):
    n_folds = 10
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)
    rmse = np.sqrt(-cross_val_score(model, df_train, y, scoring="neg_mean_squared_error", cv=kf))
    return rmse


models = [lasso, ENet, KRR, GBoost, model_xgb, model_lgb]

scores = []
for model in models:
    scores.append(rmse_cv(model))
scores = np.array(scores)
scores

df_scores = pd.DataFrame([scores.mean(axis=1), scores.std(axis=1)],
                         columns=['lasso', 'ENet', 'KRR', 'GBoost', 'model_xgb', 'model_lgb'], index=['Mean', 'Std'])
df_scores = df_scores.T
df_scores

"""## Stacked regressions

### Simple Mean Stacked Regression
"""


def rmse_cv(model):
    n_folds = 5
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(df_train.values)
    rmse = np.sqrt(-cross_val_score(model, df_train, y, scoring="neg_mean_squared_error", cv=kf))
    return rmse


import copy


class MeanStackedRegression(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, models):
        self.models = models

    def fit(self, X, y):
        self.models_ = copy.deepcopy(self.models)

        for model in self.models_:
            model.fit(X, y)

        return self

    def predict(self, X):
        predictions = np.column_stack([model.predict(X) for model in self.models_])
        return predictions.mean(axis=1)


meanModel = MeanStackedRegression(models=(ENet, GBoost, KRR, lasso))

score = rmse_cv(meanModel)
print(" Averaged base models score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

"""## Meta Model"""

import copy


class MetaStackedRegression(BaseEstimator, RegressorMixin, TransformerMixin):
    def __init__(self, models, meta_model):
        self.models_ = copy.deepcopy(models)
        self.meta_model_ = copy.deepcopy(meta_model)

    def fit(self, X, y):
        n_folds = 5
        kf = KFold(n_folds, shuffle=True, random_state=42)
        predictions = np.zeros([X.shape[0], len(self.models_)])
        for i, model in enumerate(models_):
            for train_index, holdout_index in kf.split(X, y):
                model.fit(X.loc[train_index, :], y[train_index])
                y_pred = model.predict(X.loc[holdout_index])
                predictions[holdout_index, i] = y_pred

            meta_model_.fit(predictions, y)

        return self
    def predict(self,X):
        meta_features = np.column_stack([
            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)
            for base_models in self.base_models_ ])
        return self.meta_model_.predict(predictions)


base_models = (ENet, GBoost, KRR)
meta_model = lasso

metaModel = MetaStackedRegression(base_models, meta_model)

score = rmse_cv(metaModel)
print(" Averaged base models score: {:.4f} ({:.4f})\n".format(score.mean(), score.std()))

base_models = (ENet, GBoost, KRR)
meta_model = lasso
n_folds = 5

base_models_ = copy.deepcopy(base_models)
meta_model_ = copy.deepcopy(lasso)
n_folds_ = n_folds
kf = KFold(n_folds_, shuffle=True, random_state=42)

X = df_train
y = y

predictions = np.zeros([X.shape[0], len(base_models_)])
for i, model in enumerate(base_models_):
    for train_index, holdout_index in kf.split(X, y):
        model.fit(X.loc[train_index, :], y[train_index])
        y_pred = model.predict(X.loc[holdout_index])
        predictions[holdout_index, i] = y_pred

meta_model.fit(predictions, y)

pd.DataFrame(predictions)

meta_model.predict(predictions)

"""## Testando na regressÃ£o Linear"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_train, y, test_size=0.33, random_state=42)

from sklearn.linear_model import LinearRegression

lm = LinearRegression()
lm.fit(RobustScaler().fit_transform(X_train), y_train)

lm.predict(X_train)
y_train

pd.DataFrame([lm.predict(X_train), y_train])

plt.scatter(lm.predict(X_train), y_train)

np.sqrt(mean_squared_error(lm.predict(X_test), y_test))

pd.DataFrame([lm.predict(X_test), y_test])

mean_squared_error(lm.predict(X_test), y_test)

sum(lm.predict(X_test) - y_test) ** 2 / len(X_test)

model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468,
                             learning_rate=0.05, max_depth=3,
                             min_child_weight=1.7817, n_estimators=2200,
                             reg_alpha=0.4640, reg_lambda=0.8571,
                             subsample=0.5213, silent=1,
                             random_state=7, nthread=-1)

model_xgb.fit(X_train, y_train)

plt.scatter(model_xgb.predict(X_test), y_test)

mean_squared_error(model_xgb.predict(X_test), y_test)

lasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))
lasso.fit(X_train, y_train)

plt.scatter(model_xgb.predict(X_test), y_test)
print(mean_squared_error(model_xgb.predict(X_test), y_test))

kf = KFold(10, shuffle=True, random_state=42).get_n_splits(df_train.values)
cross_val_score(model_xgb, df_train, y, cv=kf).mean()

# Applying K-Fold Cross Validation
from sklearn.model_selection import cross_val_score

accuracies = cross_val_score(estimator=model_xgb, X=df_train, y=y, cv=10)
accuracies.mean()
